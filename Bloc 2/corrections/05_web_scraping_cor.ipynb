{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5 : Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décommenter la ligne suivante pour installer lxml (nécessaire pour read_html)\n",
    "# %pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonsaïs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le site [Umi Zen Bonsai](https://umizenbonsai.com/) est une boutique de vente en ligne dédiée aux bonsaïs. Les conifères sont disponible sur la page web [https://umizenbonsai.com/shop/bonsai/coniferes/](https://umizenbonsai.com/shop/bonsai/coniferes/). Comme beaucoup d'autres sites, l'information est organisée en blocs dans lesquels il est possible de récupérer des données.\n",
    "\n",
    "Pour scraper ce type de site, le processus consiste à capturer les blocs dans un premier temps, puis à en extraire les données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Récupérer le contenu de la page avec `requests` et passer le résultat au parser de `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bonsai = \"https://umizenbonsai.com/shop/bonsai/coniferes/\"\n",
    "\n",
    "r_bonsai = requests.get(url_bonsai)\n",
    "\n",
    "if r_bonsai.status_code != 200:\n",
    "    print(f\"Erreur {r_bonsai.status_code}\")\n",
    "\n",
    "soup_bonsai = BeautifulSoup(r_bonsai.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Écrire un sélecteur CSS pour capturer les éléments `li` qui contiennent les blocs correspondants aux bonsaïs. Vérifier sur le site que le nombre de bonsaïs affichés correspond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les éléments li à récupérer ont tous une classe \"entry\".\n",
    "selector_bonsai = \"li.entry\"\n",
    "bonsai_list = soup_bonsai.select(selector_bonsai)\n",
    "\n",
    "print(f\"{len(bonsai_list)} bonsaïs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Écrire une fonction qui prend un bloc de la liste précédente et retourne un tuple contenant le nom, le prix et le lien de description du bonsaï."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonsai_info(bonsai):\n",
    "    # Toutes les données sont dans une sous-liste à puce ul\n",
    "    # Le sélecteur CSS est relatif à l'élément li transmis\n",
    "    data = bonsai.select_one(\"div ul\")\n",
    "\n",
    "    # Nom et lien du bonsaï\n",
    "    titre = data.select_one(\"li.title h2 a\")\n",
    "    nom = titre.text\n",
    "    url = titre.attrs[\"href\"]\n",
    "\n",
    "    # Prix du bonsaï\n",
    "    prix = data.select_one(\"li.price-wrap span span bdi\").text\n",
    "\n",
    "    return (nom, prix, url)\n",
    "\n",
    "bonsai_info(bonsai_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Utiliser les deux questions précédentes pour construire un dataframe contenant les données des bonsaïs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonsais_data = [bonsai_info(bonsai) for bonsai in bonsai_list]\n",
    "bonsais = pd.DataFrame(\n",
    "    bonsais_data,\n",
    "    columns=[\"Nom\", \"Prix\", \"URL\"]\n",
    ")\n",
    "\n",
    "bonsais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. (*Bonus*) Écrire une fonction pour récupérer la provenance, le feuilage et les dimension du bonsaï à partir du lien de description. Utiliser cette fonction pour ajouter des colonnes au dataframe précédent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonsai_details(url):\n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Erreur {r.status_code}\")\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    data = soup.select(\"div.elementor-widget-container p\")\n",
    "\n",
    "    # L'attribut stripped_strings permet de découper les retours à la ligne <br/>\n",
    "    dim = list(data[5].stripped_strings)\n",
    "    \n",
    "    return pd.Series({\n",
    "        \"Provenance\": data[0].text,\n",
    "        \"Feuillage\": data[1].text,\n",
    "        \"Nebari\": dim[0],\n",
    "        \"Hauteur\": dim[1],\n",
    "    })\n",
    "\n",
    "# Avec axis=1, la fonction concat accumule les colonnes de deux dataframes\n",
    "pd.concat(\n",
    "    [\n",
    "        bonsais,\n",
    "        bonsais.apply(lambda row: bonsai_details(row.URL), axis=1)\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trampoline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le trampoline est un sport olympique depuis les jeux de Sydney en 2000. La page suivante contient les listes des hommes et des femmes ayant obtenu une médaille olympique dans cette discipline :\n",
    "[https://fr.wikipedia.org/wiki/Liste_des_m%C3%A9daill%C3%A9s_olympiques_au_trampoline](https://fr.wikipedia.org/wiki/Liste_des_m%C3%A9daill%C3%A9s_olympiques_au_trampoline)\n",
    "\n",
    "Un tableau est contenu dans un élément `table` avec des balises pour les lignes `tr`, pour les colonnes `th`, pour les cellules `td`, ... Cela peut être fastidieux à scraper et très répétitif. Heureusement, Pandas propose la fonction `read_html` pour récupérer des tableaux sous forme de dataframes à partir d'une page web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Utiliser la fonction `read_html` de Pandas sur la page des médaillés olympiques au trampoline. Combien de dataframes sont récupérés ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trampoline_url = \"https://fr.wikipedia.org/wiki/Liste_des_m%C3%A9daill%C3%A9s_olympiques_au_trampoline\"\n",
    "trampoline_dfs = pd.read_html(trampoline_url)\n",
    "\n",
    "print(f\"{len(trampoline_dfs)} dataframes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Extraire de la liste précédente les dataframes des médailles masculines et féminines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trampoline_homme = trampoline_dfs[0]\n",
    "trampoline_femme = trampoline_dfs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. À partir de ces dataframes, compter combien chaque pays a reçu de médailles d'or, d'argent et de bronze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'expression régulière \"\\((.*)\\)\" correspond au texte entre parenthèses.\n",
    "# La méthode value_count compte les occurences des valeurs distinctes.\n",
    "# La méthode add permet de remplacer les données manquantes par des zéros.\n",
    "\n",
    "# Médailles Or\n",
    "or_homme = trampoline_homme.Or.str.extract(\"\\((.*)\\)\").value_counts()\n",
    "or_femme = trampoline_femme.Or.str.extract(\"\\((.*)\\)\").value_counts()\n",
    "or_medailles = or_homme.add(or_femme, fill_value=0)\n",
    "\n",
    "or_medailles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Médailles Argent\n",
    "argent_homme = trampoline_homme.Argent.str.extract(\"\\((.*)\\)\").value_counts()\n",
    "argent_femme = trampoline_femme.Argent.str.extract(\"\\((.*)\\)\").value_counts()\n",
    "argent_medailles = argent_homme.add(argent_femme, fill_value=0)\n",
    "\n",
    "argent_medailles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Médailles Bronze\n",
    "bronze_homme = trampoline_homme.Bronze.str.extract(\"\\((.*)\\)\").value_counts()\n",
    "bronze_femme = trampoline_femme.Bronze.str.extract(\"\\((.*)\\)\").value_counts()\n",
    "bronze_medailles = bronze_homme.add(bronze_femme, fill_value=0)\n",
    "\n",
    "bronze_medailles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (*Bonus*) Construire un dataframe contenant, pour chaque pays, le nombre de médailles d'or, d'argent et de bronze ainsi que le nombre total de médailles. Classer ce dataframe dans l'ordre usuel en fonction d'abord du nombre de médailles d'or, puis du nombre de médailles d'argent et enfin du nombre de médailles de bronze. Comparer le résultat avec le tableau des médailles sur la page [https://fr.wikipedia.org/wiki/Trampoline_aux_Jeux_olympiques](https://fr.wikipedia.org/wiki/Trampoline_aux_Jeux_olympiques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténation des colonnes\n",
    "medailles = (\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"Or\": or_medailles,\n",
    "            \"Argent\": argent_medailles,\n",
    "            \"Bronze\": bronze_medailles,\n",
    "        }\n",
    "    )\n",
    "    .rename_axis(\"Pays\")\n",
    "    .fillna(0)\n",
    "    .assign(Total=lambda row: row.Or + row.Argent + row.Bronze)\n",
    "    .sort_values(by=[\"Or\", \"Argent\", \"Bronze\"], ascending=False)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "medailles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cate Blanchett"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cours, nous avons essayé de trouver avec quels acteurs Cate Blanchett a joué le plus au cours des années 2000. Pour cela, nous avons récupéré la liste des pages Wikipedia des films où elle tient un rôle avec le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_wikipedia = \"https://fr.wikipedia.org\"\n",
    "url_blanchett = url_wikipedia + \"/wiki/Cate_Blanchett\"\n",
    "\n",
    "r_blanchett = requests.get(url_blanchett)\n",
    "assert r_blanchett.status_code == 200, f\"Erreur {r_blanchett.status_code}\"\n",
    "\n",
    "soup_blanchett = BeautifulSoup(r_blanchett.text, \"html.parser\")\n",
    "\n",
    "selector_films = \"#mw-content-text div ul:nth-of-type(3) li i a\"\n",
    "films_blanchett = soup_blanchett.select(selector_films)\n",
    "\n",
    "films_data = [\n",
    "    {\n",
    "        \"titre\": film.attrs[\"title\"],\n",
    "        \"url_wikipedia\": url_wikipedia + film.attrs[\"href\"]\n",
    "    }\n",
    "    for film in films_blanchett\n",
    "    if not (\n",
    "        film.attrs.get(\"class\") == [\"new\"] # Film sans page\n",
    "        or film.attrs[\"title\"] == \"Galadriel\" # Mauvais lien\n",
    "    )\n",
    "]\n",
    "\n",
    "films = pd.DataFrame(films_data)\n",
    "\n",
    "films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le sélecteur CSS que nous avons utilisé ne permettait pas d'obtenir la réponse à notre question car il ne capturait pas toutes les listes d'acteurs (organisation différente pour *Coffee and Cigarettes*, double colonne pour *Aviator*, ...). En effet, les pages Wikipedia des films ne sont pas uniformes et il n'est pas possible d'extraire la distribution de tous les films avec le même sélecteur.\n",
    "\n",
    "Pour remédier à cela, nous proposons ici d'aller scraper la liste des acteurs sur le site [TMDB](https://www.themoviedb.org/) (*The Movie Database*) dont les pages obéissent toutes à la même organisation. Les pages Wikipedia relatives à des films contiennent toutes un lien externe vers ce site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pour chaque film, scraper la page Wikipedia pour récupérer le lien vers la page TMDB associée et déduire le lien du casting complet qui ser ajouté le dans une nouvelle colonne du dataframe `films`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_tmdb_casting(url_wikipedia):\n",
    "    # Les liens externes ont tous la classe \"external text\"\n",
    "    selector_lien_externe = \"a[class='external text']\"\n",
    "\n",
    "    r_film = requests.get(url_wikipedia)\n",
    "    assert r_film.status_code == 200, f\"Erreur {r_film.status_code}\"\n",
    "    soup_film = BeautifulSoup(r_film.text, \"html.parser\")\n",
    "    liens = soup_film.select(selector_lien_externe)\n",
    "    liens_tmdb = [\n",
    "        lien.attrs[\"href\"]\n",
    "        for lien in liens\n",
    "        if lien.text == \"The Movie Database\"\n",
    "    ]\n",
    "    assert len(liens_tmdb) == 1, \"Erreur de lien TMDB\"\n",
    "    # Il faut ajouter /cast au lien TMDB pour le casting\n",
    "    return liens_tmdb[0] + \"/cast\"\n",
    "\n",
    "films[\"url_tmdb_casting\"] = films.url_wikipedia.apply(url_tmdb_casting)\n",
    "\n",
    "films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. La liste des acteurs d'un film se présente comme une liste ordonnée `ol` dans les pages TMDB. Scraper les pages de casting pour ajouter la liste des acteurs de chaque film dans une nouvelle colonne du dataframe `films`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_acteurs(url):\n",
    "    r_casting = requests.get(url)\n",
    "    assert r_casting.status_code == 200, f\"Erreur {r_casting.status_code}\"\n",
    "    soup_casting = BeautifulSoup(r_casting.text, \"html.parser\")\n",
    "    # Le combinateur d'enfant direct '>' permet de limiter le sélecteur au casting\n",
    "    selector_casting = \"section > ol li div div p a\"\n",
    "    return [acteur.text for acteur in soup_casting.select(selector_casting)]\n",
    "\n",
    "films[\"acteurs\"] = films.url_tmdb_casting.apply(list_acteurs)\n",
    "\n",
    "films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Utiliser le résultat de la question précédente pour répondre à la question initiale : avec quels acteurs Cate Blanchett a-t-elle partagé l'affiche le plus souvent au cours des années 2000 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mise en commun de tous des noms des acteurs\n",
    "acteurs_list = [\n",
    "    acteur\n",
    "    for acteurs in films.acteurs.to_list()\n",
    "    for acteur in acteurs\n",
    "]\n",
    "\n",
    "# La réponse vient en comptant les occurrences de chaque acteur.\n",
    "# Nous retrouvons bien les acteurs de la trilogie du Seigneur des Anneaux :-)\n",
    "(\n",
    "    pd.Series(acteurs_list)\n",
    "    .value_counts()\n",
    "    .head(20)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
